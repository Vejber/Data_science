---
title: "Пример анализа данных"
output: html_document
author: Столярова Валерия Фуатовна
---

```{r echo=FALSE, message=FALSE}
knitr::opts_chunk$set(cache = TRUE)

library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)

winequality <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep=";")

winequality <- winequality  %>% mutate(quality = as.character(quality))
```

**Набор данных: **  были проанализированы 4898 экземпляров белого вина, определены их физико--химические свойства. Качество вина определялось экспертным методом, каждый из по крайней мере трех экспертов оценивал вино по шкале от 0 до 10. В качестве финальной оценки представлена медиана оценок экспертов.

Были измерены следующие физико--химические свойства:

   1. fixed acidity --- содержание нелетучих кислот
   2. volatile acidity --- содержание летучих кислот
   3. citric acid --- содержание аскорбиновой кислоты
   4. residual sugar --- содержание остаточных сахаров
   5. chlorides --- содержание хлоридов
   6. free sulfur dioxide --- содержание свобоного диоксида серы
   7. total sulfur dioxide --- общее содержание диоксида серы
   8. density --- плотность
   9. pH --- кислотность
   10. sulphates --- содержание сульфатов
   11. alcohol --- крепость
   12. quality --- качество


## Линейная регрессия, подгонка методом МНК (метод наименьших квадратов)

**Исследовательский вопрос:** как плотность вина может быть определена на основе других физико--химических показателей?

Основной статистической моделью такой зависимости выбирается линейная регрессия. 

```{r}
regr_data <- winequality %>% select(-quality)
rmodel_1 <- lm(density~., data=regr_data)
summary(rmodel_1)
```

Чтобы судить о валидности построенной регрессионной модели, необходимо провести ее *диагностику*. Диагностика линейной регрессии состоит из двух частей

1. **Качество подгонки модели**. Для построенной модели коэффициент детерминации `r round(summary(rmodel_1)$adj.r.squared,2)`, таким образом, подогнанная регрессия объясняет большую часть вариабельности ключевой переменной *плотности*. Кроме того, p--значение для F--критерия равно `r pf(summary(rmodel_1)$fstatistic[1], summary(rmodel_1)$fstatistic[2], summary(rmodel_1)$fstatistic[3], lower.tail = F)`, таким образом гипотеза о равенстве нулю всех коэффициентов при признаках не принимается на уровне значимости 0.05, регрессия значима.
2. **Анализ остатков регрессии** по четырем базовым пунктам
- **линейность взаимосвязи**. Истинность предположения о линейности взаимосвязи можно определить по зависимости между подогнанными значениями и остатками регрессии --- на этом графике не должно наблюдаться никакого тренда и выбросов, среднее значение остатков регрессии --- 0. Для рассматриваемой модели можно говорить о наличии выбросов, это примерно наблюдения с номерами `r which(abs(as.numeric(rmodel_1$residuals)) > 0.003) `.
```{r}
#можно вывести этот plot при помощи базовой графики
#plot(rmodel_1, 1)

p1<-ggplot(rmodel_1, aes(.fitted, .resid))+ geom_point() + stat_smooth(method="loess") + geom_hline(yintercept=0, col="red", linetype="dashed") + 
labs(x="Подогнанные значения (fitted)", y="Остатки регрессии (residuals)", title = "Residual vs Fitted Plot")+theme_bw()

p1
```

Для более полного анализа выбросов, используется расстояние Кука (Cook's distance), которое отражает, насколько сильно изменится предсказание при удалении каждого наблюдения. Обычно выбросами считаются те наблюдения, для которых расстояние Кука больше, чем $4/n$, или 0.0008 в нашем случае. 

```{r}
plot(rmodel_1, 5)

#или

p5 <- ggplot(rmodel_1, aes(.hat, .stdresid)) + 
  geom_point(aes(size=.cooksd), na.rm=TRUE) + 
  stat_smooth(method="loess", na.rm=TRUE) + 
  labs(x="Leverage", y="Standardized Residuals", title = "Residual vs Leverage Plot") + 
  scale_size_continuous("Cook's Distance", range=c(1,5)) + 
  theme_bw()+theme(legend.position="bottom")

#создадим индекс, который определяет соответствие наблюдения эпирическому правилу о выбросах
inx <- ifelse(cooks.distance(rmodel_1) < 4/4898, "keep","delete")

p5
```



```{r}
#оставим в модели только те наблюдения, которые не считаются выбросами
rmodel_1_mod <- lm(density~., data=regr_data[inx=='keep',])
summary(rmodel_1_mod)
```

- **нормальность остатков регрессии**: соответствие остатков регрессии нормальному распределению вероятности может быть проверено при помощи статистических тестов или графика типа квантиль--квантиль (для больших выборок этот шаг используется для описания особенностей данных, выбросов; статистические выводы верны и при разумных отклонениях от нормальности). Для подогнанной регрессии график квантиль--квантиль отражает близость остатков нормальному распределению (за исключением правого хвоста), однако согласно критерию Шапиро--Уилка соответствующая гипотеза не может быть принята на уровне значимости 0.05. Отметим, что чем меньше выборка, тем более жестким является требование нормальности остатков, для больших выборок это не является обязательным.
```{r}
#критерий Лиллиефорса для проверки на нормальность из пакета nortest
nortest::lillie.test(rstandard(rmodel_1_mod))
```
```{r}
p2 <- ggplot(rmodel_1_mod, aes(qqnorm(.stdresid)[[1]], .stdresid)) + 
  geom_point(na.rm = TRUE) + 
  geom_abline() + 
  labs(x = "Нормальное распределение", y = "Стандартизованные остатки", title = "График Q-Q с нормальным распределением") + 
  theme_bw()

p2
```

- **однородность дисперсии остатков**, гомоскедастичность, может быть оценена при помощи статистических критериев или же графически. Это свойство является также желательным, но не обязательным. Чем меньше выборка, тем более жестким является требование гомоскедастичности остатков. В нашем случае p-значение для критерия меньше 0.05, и потому гипотеза об однородности дисперсии не принимается, хотя графически значительных отклонений не выявляется. 

```{r}
#один из базовых критериев для проверки на гомоскедастичность
car::ncvTest(rmodel_1_mod)
```
```{r}
ggplot(rmodel_1_mod, aes(.fitted, sqrt(abs(.stdresid)))) + 
  geom_point(na.rm=TRUE) + 
  stat_smooth(method="loess", na.rm = TRUE) + 
  labs(x="Fitted Value", y = expression(sqrt("|Standardized residuals|")), title = "Scale-Location plot") + 
  theme_bw()
```

В целом, простая линейная регрессия отлично описывает интересующий признак, плотность: сильных отклонений от предпосылок модели не наблюдается, регрессия в целом значима, описывает 98% вариабельности признака. Однако заметим, что плотность по своей сути не может принимать отрицательные значения, и потому корректнее будет:

- либо использовать логарифмическое преобразование признака в модели;
- либо прибегнуть к *обобщенной линейной регрессии*.


Итоговая регрессия (с учетом удаленных выбросов) позволяет охарактеризовать плотность в терминах других физических характеристик вина следующим образом:

**Интерпретация коэффициентов линейной регрессии**

- увеличение показателя fixed.acidity на 1 влечет увеличение плотности вина на 0.0008.
- увеличение показателя volatile.acidity на 1 влечет увеличение плотности вина на 0.0002.
- увеличение показателя citric.acid на 1 влечет увеличение плотности вина на 0.0002.
- увеличение показателя residual.sugar на 1 влечет увеличение плотности вина на 0.0004.
- увеличение показателя chlorides на 1 влечет увеличение плотности вина на 0.0046.
- увеличение показателя free.sulphur.dioxide на 1 влечет уменьшение плотности вина на 0.000006.
- увеличение показателя total.sulphur.dioxide на 1 влечет увеличение плотности вина на 0.000004.
- увеличение показателя pH на 1 влечет увеличение плотности вина на 0.0032.
- увеличение показателя sulphates на 1 влечет увеличение плотности вина на 0.0013.
- увеличение показателя alchohol на 1 влечет снижение плотности вина на 0.0011.


## Обобщенная линейная регрессия, подгонка методом ММП (метод максимума правдоподобия)

```{r}
rmodel_2 <- glm(density~., data=regr_data[inx=="keep",], family="inverse.gaussian")
summary(rmodel_2)
```
Для оценки значимости регрессии в целом используется критерий отношения правдоподобия (аналог F--критерия для подгонки регресии МНК), который сравнивает правдоподобие подогнанной модели с нулевой моделью (без признаков).
```{r}
#Критерий отношения правдоподобия из пакета lmtest
lmtest::lrtest(rmodel_2)
```

Регрессия в целом значима, коэффициенты при признаках также значимы.

Остатки регресси при подгонке методом ММП имеют несколько другую интерпретацию (остатки можно вычислять по-разному), однако всегда важно посмотреть на влиятельные наблюдения (расстояние Кука)

```{r}
plot(rmodel_2,5)
```



Анализ графика позволяет сказать, что влиятельных точек нет, хотя согласно правилу, что расстояние Кука должно быть меньше $4/4898$, выбросами считаются `r length(which(cooks.distance(rmodel_2) > 4/4898))` наблюдений. 


## Отбор ключевых признаков при помощи регрессии LASSO

Предположим, что датасет был собран небольшой, и данных для подгонки регрессии классическими методами не хватает. В этом случае можно прибегнуть к отбору релевантных признаков при помощи регуляризованной регрессии.

```{r}
set.seed(56635)
regr_data_small <- regr_data[sample(1:4858, 50), ]
```


```{r}
library(glmnet)

y <- log(regr_data_small$density)

#define matrix of predictor variables
x <- data.matrix(regr_data_small %>% select(-density))

cv_model <- cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)

coef(best_model)
```

```{r}
rmodel_3 <- lm(log(density) ~ fixed.acidity + citric.acid + residual.sugar + total.sulfur.dioxide + pH + alcohol, data=regr_data_small)

summary(rmodel_3)
```

## Предсказательное качество регрессионной модели

Для демонстрации анализа предсказательной способности регрессионной модели, ввыедем новую дихотомическую переменную, которая определяет качество вина: 0 --- нормально вино, качество от 3 до 5; 1 --- хорошее вино, оценка качества выше 6.

```{r}
set.seed(42)

regr_data_binq <- winequality %>% mutate(binq = cut(as.numeric(quality), c(2.5, 5.5, 9.5), labels=c(0, 1)), quality=NULL)

# для оценки качества предсказаний, необходимо ввести тестовую и тренировочную подвыборки. На тренировочной выборке подгоняется модель; на тестовой --- оценивается качество ее работы.
sample_split <- caTools::sample.split(regr_data_binq$binq, SplitRatio = .7)
train_set <- subset(x = regr_data_binq, sample_split == TRUE)
test_set <- subset(x = regr_data_binq, sample_split == FALSE)

#логистическая регрессия
logistic_1 <- glm(binq ~ ., data = train_set, family = "binomial")
summary(logistic_1)
```

Есть коэффициенты, которые статистически значимо не отличаются от нуля. Следуя принципу экономичного построения моделей, уберем их. Кроме того, обратим внимание на слишком большое значение коэффициента $\beta_0$, или intercept. При использовании предсказательной модели его необходимо будет экспонировать. Такие огромные значение коэффициента говорят об "идеальной разделимости" (complete separation) в логистической регрессии. По очереди удаляя переменные из логистиеческой регрессии можно понять, что проблемы вызывает признак density, который коррелирован со многими другими переменными в модели. Также не будем рассматривать его при анализе.

```{r}
logistic_2 <- glm(binq ~ ., data = train_set %>% select(-fixed.acidity, -citric.acid, -chlorides, -total.sulfur.dioxide, -density, -pH), family = "binomial")
summary(logistic_2)

test_set <- test_set %>% select(-fixed.acidity, -citric.acid, -chlorides, -total.sulfur.dioxide, -density, -pH)

```

Вторая модель имеет меньшее число параметров, при этом имеет более высокое правдоподобие (согласно критерию отношения правдоподобия), поэтому является предпочтительной.

```{r}
lmtest::lrtest(logistic_1, logistic_2)
```

**Интерпретация коэффициентов логистической регрессии**

- увеличение показателя volatile.acidity на 1 влечет увеличение плотности вина на `r round(exp(logistic_2$coefficients)[2], 4)`.
- увеличение показателя residual.sugar на 1 влечет увеличение плотности вина на `r round(exp(logistic_2$coefficients)[3], 4)`.
- увеличение показателя free.sulphur.dioxide на 1 влечет уменьшение плотности вина на `r round(exp(logistic_2$coefficients)[4], 4)`.
- увеличение показателя sulfates на 1 влечет увеличение плотности вина на `r round(exp(logistic_2$coefficients)[5], 4)`.
- увеличение показателя alcohol на 1 влечет увеличение плотности вина на `r round(exp(logistic_2$coefficients)[6], 4)`.

Для оценки качества предсказаний, используем тестовый набор данных, и оценим метрики классификации для получаемых при помощи логистической регрессии предсказаний по отношению к истинным значениям.

```{r}
#предсказание вероятности исхода (вероятности, что вино отличное) на тестовом датасете
probs <- predict(logistic_2, newdata = test_set, type = "response")

#определение класса вина (хорошее или отличное) на основании вероятности, отсечкой служит значение 0.5, но можно выбирать и другие.
pred <- ifelse(probs > 0.5, 1, 0)

caret::confusionMatrix(factor(pred), factor(test_set$binq), positive = as.character(1))

```

Интепретация: модель обладает средними характеристиками качества классификации вин по их качеству.